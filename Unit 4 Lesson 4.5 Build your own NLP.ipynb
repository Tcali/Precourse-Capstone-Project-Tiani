{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Supervised NLP requires a pre-labelled dataset for training and testing, and is generally interested in categorizing text in various ways. In this case, we are going to try to predict whether a sentence comes from _Alice in Wonderland_ by Lewis Carroll or _Persuasion_ by Jane Austen. We can use any of the supervised models we've covered previously, as long as they allow categorical outcomes. In this case, we'll try Random Forests, SVM, and KNN.\n",
    "\n",
    "Our feature-generation approach will be something called _BoW_, or _Bag of Words_. BoW is quite simple: For each sentence, we count how many times each word appears. We will then use those counts as features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Utility function for standard text cleaning.\n",
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "    \n",
    "# Load and clean the data.\n",
    "bible = gutenberg.raw('bible-kjv.txt')\n",
    "hamlet = gutenberg.raw('shakespeare-hamlet.txt')\n",
    "bb = gutenberg.raw('burgess-busterbrown.txt')\n",
    "\n",
    "# The Chapter indicator is idiosyncratic\n",
    "bible = re.sub(r'Chapter \\d+', '', bible)\n",
    "hamlet = re.sub(r'Chapter \\d+', '', hamlet)\n",
    "bb = re.sub(r'Chapter \\d+', '', bb)\n",
    "    \n",
    "hamlet = text_cleaner(hamlet)\n",
    "bible = text_cleaner(bible)\n",
    "bb = text_cleaner(bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3384, 5140)\n"
     ]
    }
   ],
   "source": [
    "# spacy is bad for you do this the right way\n",
    "# We need to do the following:\n",
    "# 1) Compute lemmatization for each text & get all the unique lemmas\n",
    "# 2) Get the set of common terms across all texts\n",
    "# 3) Do count vectorization to remove stop words and get a bag-of-words matrix\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "def lemmatize(text):\n",
    "    \n",
    "    # Break up text into sentences and then split each sentence into a list of words\n",
    "    sentenceEnders = re.compile('[.!?]')\n",
    "    sentenceList = sentenceEnders.split(text)\n",
    "    word_list = [s.split(' ') for s in sentenceList]\n",
    "\n",
    "    # Do lemmatization, lower-case conversion, number/punctuation stripping in one shot\n",
    "    translator_punct = str.maketrans('', '', string.punctuation)\n",
    "    translator_numbers = str.maketrans('', '', '0123456789')\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizer_split = [[wordnet_lemmatizer.lemmatize(w.lower().translate(translator_punct).translate(translator_numbers)) \n",
    "                        for w in s] for s in word_list]\n",
    "    \n",
    "    # Now we compute the set of unique words in the text\n",
    "    # To do this, we recombine all the lemmas together and then convert to a set to get unique\n",
    "    sentence_recombined = [\" \".join(s) for s in lemmatizer_split]\n",
    "    text_recombined = \" \".join(sentence_recombined)\n",
    "    unique_word_list = list(set(text_recombined.split(\" \")))\n",
    "    unique_word_list = [u for u in unique_word_list if len(u)>0]\n",
    "    return sentence_recombined, unique_word_list\n",
    "\n",
    "# Run alice and persuasion through the lemmatizer\n",
    "bb_lemmas, bb_words = lemmatize(bb)\n",
    "hamlet_lemmas, hamlet_words = lemmatize(hamlet)\n",
    "all_sentences = bb_lemmas + hamlet_lemmas\n",
    "\n",
    "# Now do count vectorization with stop word removal\n",
    "cv = CountVectorizer(stop_words='english')  # Will automatically compute unique vocab\n",
    "X = cv.fit_transform(all_sentences).toarray()\n",
    "y = ['Burgess'] * len(bb_lemmas) + ['Shakespeare'] * len(hamlet_lemmas)\n",
    "print(X.shape) # Number of sentences by number of vocab terms\n",
    "vocab_dict = cv.vocabulary_ # Gets you a mapping of term to column index\n",
    "\n",
    "# Let's weave into a final dataframe\n",
    "X_df = pd.DataFrame(X)\n",
    "X_df['author'] = y\n",
    "# Can set the columns to the elements of vocab_dict in the correct order if you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Time to bag some words!  Since spaCy has already tokenized and labelled our data, we can move directly to recording how often various words occur.  We will exclude stopwords and punctuation.  In addition, in an attempt to keep our feature space from exploding, we will work with lemmas (root words) rather than the raw text terms, and we'll only use the 2000 most common words for each text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Trying out BoW\n",
    "\n",
    "Now let's give the bag of words features a whirl by trying a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9892870336165497\n",
      "\n",
      "Test set score: 0.9202363367799113\n"
     ]
    }
   ],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "# y = X_df['author']\n",
    "X = np.array(X_df.drop(['author'], 1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0)\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## BoW with Logistic Regression\n",
    "\n",
    "Let's try a technique with some protection against overfitting due to extraneous features â€“ logistic regression with ridge regularization (from ridge regression, also called L2 regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9737717029922424\n",
      "\n",
      "Test set score: 0.9261447562776958\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "# print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Logistic regression performs a bit better than the random forest.  \n",
    "\n",
    "# BoW with Gradient Boosting\n",
    "\n",
    "And finally, let's see what gradient boosting can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9009974141115626\n",
      "\n",
      "Test set score: 0.8670605612998523\n"
     ]
    }
   ],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Looks like logistic regression is the winner, but there's room for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95427729, 0.92466765, 0.93057607, 0.91715976, 0.92159763])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(lr, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[ The Tragedie of Hamlet by William Shakespeare 1599 ]', 'Actus Primus .', 'Enter Barnardo and Francisco two Centinels .', 'Barnardo .']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# reading in the data, this time in the form of paragraphs\n",
    "hamlet = gutenberg.paras('shakespeare-hamlet.txt')\n",
    "# processing\n",
    "hamlet_paras = []\n",
    "for paragraph in hamlet:\n",
    "    para = paragraph[0]\n",
    "    # removing the double-dash from all words\n",
    "    para = [re.sub(r'--', '', word) for word in para]\n",
    "    # Forming each paragraph into a string and adding it to the list of strings.\n",
    "    hamlet_paras.append(' '.join(para))\n",
    "\n",
    "print(hamlet_paras[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 40\n",
      "Original sentence: Mar .\n",
      "Tf_idf vector: {'mar': 1.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(hamlet_paras, test_size=0.3, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than 50% of  the paragraphs\n",
    "                             min_df=3, # only use words that appear at least 3x\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "hamlet_paras_tfidf=vectorizer.fit_transform(hamlet_paras)\n",
    "print(\"Number of features: %d\" % hamlet_paras_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(hamlet_paras_tfidf, test_size=0.3, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 99.92201080577465\n",
      "Component 0:\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Ham .    1.0\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "King .    0.999651\n",
      "King .    0.999651\n",
      "King .    0.999651\n",
      "King .    0.999651\n",
      "King .    0.999651\n",
      "King .    0.999651\n",
      "King .    0.999651\n",
      "King .    0.999651\n",
      "King .    0.999651\n",
      "King .    0.999651\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "Hor .    1.0\n",
      "Hor .    1.0\n",
      "Hor .    1.0\n",
      "Hor .    1.0\n",
      "Hor .    1.0\n",
      "Hor .    1.0\n",
      "Hor .    1.0\n",
      "Hor .    1.0\n",
      "Hor .    1.0\n",
      "Hor .    1.0\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "Laer .    0.999848\n",
      "Laer .    0.999848\n",
      "Laer .    0.999848\n",
      "Laer .    0.999848\n",
      "Laer .    0.999848\n",
      "Laer .    0.999848\n",
      "Laer .    0.999848\n",
      "Laer .    0.999848\n",
      "Laer .    0.999848\n",
      "Laer .    0.999848\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "Qu .    1.0\n",
      "Qu .    1.0\n",
      "Qu .    1.0\n",
      "Qu .    1.0\n",
      "Qu .    1.0\n",
      "Qu .    1.0\n",
      "Qu .    1.0\n",
      "Qu .    1.0\n",
      "Qu .    1.0\n",
      "Qu .    1.0\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 40to 39.\n",
    "svd= TruncatedSVD(39)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:\n",
      "0 Ham .\n",
      "1 Rosin .\n",
      "2 Qu .\n",
      "3 Ham .\n",
      "4 Manet Hamlet .\n",
      "5 Mar .\n",
      "6 Guild .\n",
      "7 Enter King , Queene , Rosincrane , and Guildensterne Cum alijs .\n",
      "8 Other .\n",
      "9 Qu .\n"
     ]
    }
   ],
   "source": [
    "# Compute document similarity using LSA components\n",
    "similarity = np.asarray(np.asmatrix(X_train_lsa) * np.asmatrix(X_train_lsa).T)\n",
    "#Only taking the first 10 sentences\n",
    "sim_matrix=pd.DataFrame(similarity,index=X_train).iloc[0:10,0:10]\n",
    "#Making a plot\n",
    "ax = sns.heatmap(sim_matrix,yticklabels=range(10))\n",
    "plt.show()\n",
    "\n",
    "#Generating a key for the plot.\n",
    "print('Key:')\n",
    "for i in range(10):\n",
    "    print(i,sim_matrix.index[i])\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "49px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
