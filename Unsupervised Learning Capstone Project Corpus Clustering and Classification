{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit 4 Capstone for Unsupervised Learning: Corpus Clustering and Classification\n",
    "\n",
    "Purpose:  \n",
    "Import 10 texts from 10 different authors and determine the best unsupervised learning methods to identify/predict the correct author given a subset of words.\n",
    "\n",
    "Methodology: Import 10 texts and clean them. Then reduce the text into paragraphs, isolate the 2000 most common vocabulary words, and combine them all into a large dataset. Then, identify which of the clustering methods learned in this course are able to determine the author of the works the best. Next, we would identify the highest accuracy of info using the supervised learning regresison models, checking against the known authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display\n",
    "plt.style.use('bmh')\n",
    "# For clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import pairwise_distances\n",
    "# For NLP\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import string\n",
    "# For Regression Analysis\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the data, this time in the form of paragraphs\n",
    "persuasion = gutenberg.paras('austen-persuasion.txt')\n",
    "alice = gutenberg.paras('carroll-alice.txt')\n",
    "stories = gutenberg.paras('bryant-stories.txt')\n",
    "bb = gutenberg.paras('burgess-busterbrown.txt')\n",
    "thur = gutenberg.paras('chesterton-thursday.txt')\n",
    "parents = gutenberg.paras('edgeworth-parents.txt')\n",
    "macbeth = gutenberg.paras('shakespeare-macbeth.txt')\n",
    "leaves = gutenberg.paras('whitman-leaves.txt')\n",
    "poems = gutenberg.paras('blake-poems.txt')\n",
    "moby = gutenberg.paras('melville-moby_dick.txt')\n",
    "\n",
    "# processing\n",
    "def process(text):\n",
    "    text_para = []\n",
    "    for paragraph in text:\n",
    "        para = paragraph[0]\n",
    "        # removing the double-dash from all words\n",
    "        para = [re.sub(r'--', '', word) for word in para]\n",
    "        para = [re.sub(\"[\\[].*?[\\]]\", \"\", word)for word in para]\n",
    "        # The Chapter indicator is idiosyncratic\n",
    "        para = [re.sub(r'Chapter \\d+', '', word) for word in para]\n",
    "        para = [re.sub(r'CHAPTER .*', '', word) for word in para]\n",
    "        # Forming each paragraph into a string and adding it to the list of strings.\n",
    "        text_para.append(' '.join(para))\n",
    "    return text_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run texts through the processor\n",
    "persuasion_para = process(persuasion)\n",
    "alice_para = process(alice)\n",
    "stories_para = process(stories)\n",
    "bb_para = process(bb)\n",
    "thur_para = process(thur)\n",
    "parents_para = process(parents)\n",
    "macbeth_para = process(macbeth)\n",
    "leaves_para = process(leaves)\n",
    "poems_para = process(poems)\n",
    "moby_para = process(moby)\n",
    "\n",
    "all_para = persuasion_para + alice_para + stories_para + bb_para + \\\n",
    "    thur_para + parents_para + macbeth_para + \\\n",
    "    leaves_para + poems_para + moby_para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do count vectorization with stop word removal\n",
    "# Will automatically compute unique vocab\n",
    "cv = CountVectorizer(stop_words='english', max_features=2000)\n",
    "y = ['Carrol'] * len(alice_para) + ['Austen'] * len(persuasion_para) + ['Bryant'] * len(stories_para) + ['Burgess'] * len(bb_para) + ['Chesterton'] * len(thur_para) + \\\n",
    "    ['Edgeworth'] * len(parents_para) + ['Shakespeare'] * len(macbeth_para) + ['Whitman'] * \\\n",
    "    len(leaves_para) + ['Blake'] * \\\n",
    "    len(poems_para) + ['Melville'] * len(moby_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14556, 2000)\n"
     ]
    }
   ],
   "source": [
    "X = cv.fit_transform(all_para).toarray()\n",
    "print(X.shape)  # Number of sentences by number of vocab terms\n",
    "vocab_dict = cv.vocabulary_  # Gets you a mapping of term to column index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do later\n",
    "# alice = None\n",
    "# persuasion = None\n",
    "# alice_lemmas = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Normalize the data.\n",
    "X_norm = normalize(X)\n",
    "\n",
    "# Reduce it to two components.\n",
    "X_pca_2 = PCA(2).fit_transform(X_norm)\n",
    "X_pca_10 = PCA(10).fit_transform(X_norm)\n",
    "\n",
    "# Calculate predicted values.\n",
    "y_pred = KMeans(n_clusters=10, random_state=42).fit_predict(X_pca_10)\n",
    "\n",
    "# Plot the solution.\n",
    "plt.scatter(X_pca_2[:, 0], X_pca_2[:, 1], c=y_pred)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the solution against the data.\n",
    "X_norm = None\n",
    "\n",
    "crosstab = np.zeros((len(np.unique(y)), len(np.unique(y_pred))))\n",
    "row_name = np.unique(y)\n",
    "col_name = np.unique(y_pred)\n",
    "\n",
    "for idx1, key1 in enumerate(row_name):\n",
    "    for idx2, key2 in enumerate(col_name):\n",
    "        crosstab[idx1, idx2] = len(y[(y==key1) & (y_pred==key2)])\n",
    "\n",
    "crosstab_df = pd.DataFrame(crosstab, index=row_name, columns=col_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>617.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blake</th>\n",
       "      <td>234.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bryant</th>\n",
       "      <td>669.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Burgess</th>\n",
       "      <td>177.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carrol</th>\n",
       "      <td>598.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chesterton</th>\n",
       "      <td>785.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edgeworth</th>\n",
       "      <td>2484.0</td>\n",
       "      <td>572.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Melville</th>\n",
       "      <td>2132.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shakespeare</th>\n",
       "      <td>554.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Whitman</th>\n",
       "      <td>2214.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0      1     2      3      4      5      6      7     8  \\\n",
       "Austen        617.0  259.0  17.0    0.0   15.0    6.0   58.0    6.0   5.0   \n",
       "Blake         234.0    4.0   4.0    0.0    0.0    0.0   28.0    9.0   0.0   \n",
       "Bryant        669.0  184.0  16.0    0.0    0.0    0.0  254.0   53.0   2.0   \n",
       "Burgess       177.0   10.0   2.0    0.0    0.0    0.0   60.0    4.0   3.0   \n",
       "Carrol        598.0   55.0  20.0    0.0   21.0   40.0   43.0   19.0   4.0   \n",
       "Chesterton    785.0  343.0  11.0    0.0   15.0    9.0   29.0   68.0   7.0   \n",
       "Edgeworth    2484.0  572.0  81.0    0.0   10.0  127.0  148.0  102.0  74.0   \n",
       "Melville     2132.0  127.0  32.0    0.0  136.0   90.0   55.0  177.0   8.0   \n",
       "Shakespeare   554.0    0.0   0.0  115.0    0.0    0.0    2.0    5.0   0.0   \n",
       "Whitman      2214.0   10.0   0.0    0.0    0.0    0.0   38.0  162.0   5.0   \n",
       "\n",
       "                 9  \n",
       "Austen        49.0  \n",
       "Blake          5.0  \n",
       "Bryant        16.0  \n",
       "Burgess       10.0  \n",
       "Carrol        17.0  \n",
       "Chesterton    21.0  \n",
       "Edgeworth    128.0  \n",
       "Melville      36.0  \n",
       "Shakespeare    2.0  \n",
       "Whitman       49.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crosstab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each batch will be made up of 200 data points.\n",
    "minibatchkmeans = MiniBatchKMeans(\n",
    "    init='random',\n",
    "    n_clusters=10,\n",
    "    batch_size=200)\n",
    "minibatchkmeans.fit(X_pca_10)\n",
    "\n",
    "# Add the new predicted cluster memberships to the data frame.\n",
    "predict_mini = minibatchkmeans.predict(X_pca_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing MiniBatch to ones in data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Austen</th>\n",
       "      <td>48.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>477.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blake</th>\n",
       "      <td>5.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>201.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bryant</th>\n",
       "      <td>14.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>591.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Burgess</th>\n",
       "      <td>9.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carrol</th>\n",
       "      <td>17.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>342.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chesterton</th>\n",
       "      <td>17.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>355.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>602.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Edgeworth</th>\n",
       "      <td>122.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>633.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>722.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1707.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Melville</th>\n",
       "      <td>33.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>1630.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shakespeare</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>465.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Whitman</th>\n",
       "      <td>53.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>216.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1876.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0      1      2      3      4      5      6      7      8  \\\n",
       "Austen        48.0   59.0    8.0  261.0    0.0   37.0    3.0   94.0   45.0   \n",
       "Blake          5.0   28.0    9.0    4.0    0.0    7.0    0.0   23.0    7.0   \n",
       "Bryant        14.0  253.0   56.0  186.0    0.0   20.0   10.0   44.0   20.0   \n",
       "Burgess        9.0   61.0    5.0   10.0    0.0    2.0    0.0    8.0    3.0   \n",
       "Carrol        17.0   43.0   26.0   58.0    0.0   80.0    0.0  222.0   29.0   \n",
       "Chesterton    17.0   28.0   72.0  355.0    0.0   29.0    7.0  156.0   22.0   \n",
       "Edgeworth    122.0  153.0  123.0  633.0    0.0  166.0    0.0  722.0  100.0   \n",
       "Melville      33.0   51.0  175.0  131.0    0.0  249.0  378.0   89.0   57.0   \n",
       "Shakespeare    2.0    2.0    5.0    0.0  115.0    0.0    0.0   88.0    1.0   \n",
       "Whitman       53.0   41.0  206.0   10.0    0.0    0.0   17.0  216.0   59.0   \n",
       "\n",
       "                  9  \n",
       "Austen        477.0  \n",
       "Blake         201.0  \n",
       "Bryant        591.0  \n",
       "Burgess       168.0  \n",
       "Carrol        342.0  \n",
       "Chesterton    602.0  \n",
       "Edgeworth    1707.0  \n",
       "Melville     1630.0  \n",
       "Shakespeare   465.0  \n",
       "Whitman      1876.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the MiniBatch model against our earlier one.\n",
    "print('Comparing MiniBatch to ones in data')\n",
    "crosstab = np.zeros((len(np.unique(y)), len(np.unique(predict_mini))))\n",
    "row_name = np.unique(y)\n",
    "col_name = np.unique(predict_mini)\n",
    "\n",
    "for idx1, key1 in enumerate(row_name):\n",
    "    for idx2, key2 in enumerate(col_name):\n",
    "        crosstab[idx1, idx2] = len(y[(y==key1) & (predict_mini==key2)])\n",
    "\n",
    "crosstab_df = pd.DataFrame(crosstab, index=row_name, columns=col_name)\n",
    "crosstab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide into training and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.25,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we set the bandwidth. This function automatically derives a bandwidth\n",
    "# number based on an inspection of the distances among points in the data.\n",
    "bandwidth = estimate_bandwidth(X_train, quantile=0.2, n_samples=500)\n",
    "\n",
    "# Declare and fit the model.\n",
    "ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "ms.fit(X_train)\n",
    "\n",
    "# Extract cluster assignments for each data point.\n",
    "labels = ms.labels_\n",
    "\n",
    "# Coordinates of the cluster centers.\n",
    "cluster_centers = ms.cluster_centers_\n",
    "\n",
    "# Count our clusters.\n",
    "n_clusters_ = len(np.unique(labels))\n",
    "\n",
    "print(\"Number of estimated clusters: {}\".format(n_clusters_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_y = y[0:len(labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(X_train[:, 0], X_train[:, 1], c=labels)\n",
    "# plt.show()\n",
    "\n",
    "print('Comparing the assigned categories to the ones in the data:')\n",
    "crosstab = np.zeros((len(np.unique(short_y)), len(np.unique(labels))))\n",
    "row_name = np.unique(short_y)\n",
    "col_name = np.unique(labels)\n",
    "\n",
    "for idx1, key1 in enumerate(row_name):\n",
    "    for idx2, key2 in enumerate(col_name):\n",
    "        crosstab[idx1, idx2] = len(short_y[(short_y==key1) & (labels==key2)])\n",
    "\n",
    "crosstab_df = pd.DataFrame(crosstab, index=row_name, columns=col_name)\n",
    "crosstab_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seems like the K Means Mini Batch clustering is working the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusted Rand Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016157968537690297"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.adjusted_rand_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.9009801227443437\n",
      "\n",
      "Test set score: 0.6166529266281946\n"
     ]
    }
   ],
   "source": [
    "rfc = ensemble.RandomForestClassifier()\n",
    "train = rfc.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', rfc.score(X_train, y_train))\n",
    "print('\\nTest set score:', rfc.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.8049830539525511\n",
      "\n",
      "Test set score: 0.7109095905468535\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train, y_train)\n",
    "# print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ensemble.GradientBoostingClassifier()\n",
    "train = clf.fit(X_train, y_train)\n",
    "\n",
    "print('Training set score:', clf.score(X_train, y_train))\n",
    "print('\\nTest set score:', clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54320988, 0.65785861, 0.63839286, 0.61554333, 0.58121129])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(lr, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised NLP\n",
    "Use LSA to prepare text data for classification in supervised learning. In that case, the goal would be to use LSA to arrive at a smaller set of features that can be used to build a supervised model that will classify text into pre-labelled categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 10554\n",
      "Original sentence: To this speech Mr . Somerville made no answer , but turned away to look at the bow window of a handsome new inn , which the glazier was at this instant glazing .\n",
      "Tf_idf vector: {'glazier': 0.3799373133912812, 'somerville': 0.3001458712816921, 'bow': 0.27060702750491933, 'instant': 0.26202049175557457, 'inn': 0.2919444086605154, 'window': 0.2593043801119187, 'speech': 0.28641186105575345, 'new': 0.2201892854462231, 'answer': 0.2537930159494784, 'away': 0.20032983203458532, 'look': 0.2041166476556295, 'mr': 0.19445875919531996, 'handsome': 0.3128990823610265, 'turned': 0.24241540869037054}\n"
     ]
    }
   ],
   "source": [
    "# tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "X_train, X_test = train_test_split(all_para, test_size=0.25, random_state=0)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, # drop words that occur in more than half the paragraphs\n",
    "                             min_df=2, # only use words that appear at least twice\n",
    "                             stop_words='english', \n",
    "                             lowercase=True, #convert everything to lower case (since Alice in Wonderland has the HABIT of CAPITALIZING WORDS for EMPHASIS)\n",
    "                             use_idf=True,#we definitely want to use inverse document frequencies in our weighting\n",
    "                             norm=u'l2', #Applies a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "                             smooth_idf=True #Adds 1 to all document frequencies, as if an extra document existed that used every word once.  Prevents divide-by-zero errors\n",
    "                            )\n",
    "\n",
    "\n",
    "#Applying the vectorizer\n",
    "all_paras_tfidf=vectorizer.fit_transform(all_para)\n",
    "print(\"Number of features: %d\" % all_paras_tfidf.get_shape()[1])\n",
    "\n",
    "#splitting into training and test sets\n",
    "X_train_tfidf, X_test_tfidf= train_test_split(all_paras_tfidf, test_size=0.25, random_state=0)\n",
    "\n",
    "\n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 24.470159385061947\n",
      "Component 0:\n",
      "CHAPTER 131    0.999635\n",
      "CHAPTER 56     0.999635\n",
      "CHAPTER 110    0.999635\n",
      "CHAPTER 127    0.999635\n",
      "CHAPTER 124    0.999635\n",
      "CHAPTER 87     0.999635\n",
      "CHAPTER 78     0.999635\n",
      "CHAPTER V      0.999635\n",
      "CHAPTER 82     0.999635\n",
      "CHAPTER 2      0.999635\n",
      "Name: 0, dtype: float64\n",
      "Component 1:\n",
      "Macb .    1.0\n",
      "Macb .    1.0\n",
      "Macb .    1.0\n",
      "Macb .    1.0\n",
      "Macb .    1.0\n",
      "Macb .    1.0\n",
      "Macb .    1.0\n",
      "Macb .    1.0\n",
      "Macb .    1.0\n",
      "Macb .    1.0\n",
      "Name: 1, dtype: float64\n",
      "Component 2:\n",
      "\" Oh , at Harrow ,\" said the policeman                                                                          0.887492\n",
      "\" Oh , they ,\" said the new detective contemptuously ; \" no they are not a very valuable force .                0.879312\n",
      "\" Oh , please , mighty and kindly Elephant ,\" he said , making a very low bow , \" will you do me a favour ?\"    0.875396\n",
      "\" Oh , perhaps you hav ' n ' t got any ,\" he said quickly .                                                     0.801282\n",
      "\" Oh , no ,\" said the little girl , \" those are bullfrogs , croaking .\"                                         0.794998\n",
      "' Oh , YOU sing ,' said the Gryphon .                                                                           0.775626\n",
      "\" Oh , no ,\" said Paul ; \" here are some blackberries for you ; you had better wait a little bit longer .       0.757529\n",
      "\" Oh , I do beg your pardon ,\" said the little Jackal .                                                         0.753040\n",
      "' Oh , you can ' t help that ,' said the Cat : ' we ' re all mad here .                                         0.746761\n",
      "\" Oh , I am crying because I do not know how to keep the house ,\" said Elsa .                                   0.723900\n",
      "Name: 2, dtype: float64\n",
      "Component 3:\n",
      "Oh , that ' s only nominal !    0.702203\n",
      "Oh , grassy glades !            0.702158\n",
      "\" Oh , no .                     0.702090\n",
      "\" Oh !                          0.702090\n",
      "\" Oh !                          0.702090\n",
      "\" Oh !                          0.702090\n",
      "\" Oh !                          0.702090\n",
      "\" Oh !\"                         0.702090\n",
      "\" Oh !                          0.702090\n",
      "\" Oh !                          0.702090\n",
      "Name: 3, dtype: float64\n",
      "Component 4:\n",
      "Just .    0.980841\n",
      "Just .    0.980841\n",
      "Just .    0.980841\n",
      "Just .    0.980841\n",
      "Just .    0.980841\n",
      "Just .    0.980841\n",
      "Just .    0.980841\n",
      "Just .    0.980841\n",
      "Just .    0.980841\n",
      "Just .    0.980841\n",
      "Name: 4, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Dimension reduction\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "#Our SVD data reducer.  We are going to reduce the feature space from 1232 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "#Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "paras_by_component=pd.DataFrame(X_train_lsa,index=X_train)\n",
    "for i in range(5):\n",
    "    print('Component {}:'.format(i))\n",
    "    print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: \" That ' s what he never did yet ,\" said the carpenter .\n",
      "Tf_idf vector: {'carpenter': 0.7786448870456927, 'said': 0.33553186594843437, 'did': 0.5302174146619123}\n"
     ]
    }
   ],
   "source": [
    "# same model, only with the test set data. \n",
    "#Reshapes the vectorizer output into something people can read\n",
    "X_test_tfidf_csr = X_test_tfidf.tocsr()\n",
    "\n",
    "#number of paragraphs\n",
    "n = X_test_tfidf_csr.shape[0]\n",
    "#A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "#List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "#for each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_test_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_test_tfidf_csr[i, j]\n",
    "\n",
    "#Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_test[5])\n",
    "print('Tf_idf vector:', tfidf_bypara[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 26.04558747386151\n"
     ]
    }
   ],
   "source": [
    "#Our SVD data reducer.  We are going to reduce the feature space from 1232 to 130.\n",
    "svd= TruncatedSVD(130)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the test data, then project the test data.\n",
    "X_test_lsa = lsa.fit_transform(X_test_tfidf)\n",
    "\n",
    "variance_explained=svd.explained_variance_ratio_\n",
    "total_variance = variance_explained.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n",
    "\n",
    "# #Looking at what sorts of paragraphs our solution considers similar, for the first five identified topics\n",
    "# paras_by_component=pd.DataFrame(X_test_lsa,index=X_test)\n",
    "# for i in range(5):\n",
    "#     print('Component {}:'.format(i))\n",
    "#     print(paras_by_component.loc[:,i].sort_values(ascending=False)[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See if the unsupervised NLP work helped us "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.26261793533021893\n",
      "\n",
      "Test set score: 0.22423742786479803\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "train = lr.fit(X_train_lsa, y_train)\n",
    "# print(X_train.shape, y_train.shape)\n",
    "print('Training set score:', lr.score(X_train_lsa, y_train))\n",
    "print('\\nTest set score:', lr.score(X_test_lsa, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "__The best Clustering Method:__ K Means Minibatch  \n",
    "__The best Supervised Learning Regression  Model:__ Logistic Regression  \n",
    "__The best accuracy score:__   \n",
    "Logistic Regression on the paragraph-grouped text data for 10 different authors, minus stop words and punctuation, isolating the top 2000 most common vocabulary words within the paragraphs.  \n",
    "Reminder, the __Accuracy Score Results for LR:__  \n",
    "Training set score: 0.8049830539525511  \n",
    "Test set score: 0.7109095905468535  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
